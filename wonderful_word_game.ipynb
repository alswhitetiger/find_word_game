{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c42941",
   "metadata": {},
   "source": [
    "# ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸° ê²Œì„\n",
    "\n",
    "1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” ì ì ˆí•œ ë°ì´í„°ì…‹ì„ ì°¾ëŠ”ë‹¤\n",
    "2. ì›Œë“œ ì„ë² ë”© ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.\n",
    "3. ë‹¨ì–´ ìœ ì‚¬ë„ê°€ 0.8 ì´ìƒì¸ A,Bë¥¼ ëœë¤ ì¶”ì¶œí•œë‹¤\n",
    "4. A,Bì™€ ëŒ€ì‘ë˜ëŠ” Cë¥¼ ì¶”ì¶œí•œë‹¤.\n",
    "5. Dë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤.\n",
    "\n",
    "=>\n",
    "A:B = C:D ê´€ê³„ì— ëŒ€ì‘í•˜ëŠ” Dë¥¼ ì°¾ëŠ” ê²Œì„ì„ ë§Œë“ ë‹¤. <br>\n",
    "ex) A : ì‚°, B : ë°”ë‹¤, C : ë‚˜ë¬´, D :  ë¬¼\n",
    "\n",
    "**<ì¶œë ¥ ì˜ˆì‹œ>**\n",
    "\n",
    "ê´€ê³„ [ ìˆ˜ê¸ : ì¶”ë½ = ëŒ€ì‚¬ê´€ : ? ] <br>\n",
    "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°€ì¥ ì í•©í•œ ë‹¨ì–´: ì ì… <br>\n",
    "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ ëª¨ë¸ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.34 <br>\n",
    "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ìš©ëŸ‰ FastText ëª¨ë¸('cc.ko.300.bin') ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "ì´ ê³¼ì •ì€ ìˆ˜ ë¶„ ì´ìƒ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_896\\1764106501.py:9: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  model = FastText.load_fasttext_format('cc.ko.300.bin')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
      "ì •ì œëœ ëª…ì‚¬ ëª©ë¡ íŒŒì¼('filtered_noun_list.txt')ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\n",
      "ëª©ë¡ ë¡œë”© ì™„ë£Œ! ì´ 1,501,657ê°œì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "------------------------------\n",
      "ê²Œì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "\n",
      "ê´€ê³„ [ ì£¼ì°½ì„  : ê¹€ì‚¬ì„± = ê¹€ìœ ê°• : ? ]\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: ë¯¼ë³‘ì˜¤\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.00\n",
      "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\n",
      "------------------------------\n",
      "\n",
      "ê´€ê³„ [ ì´ìˆ˜ê±´ì„¤ : ì„ ì°½ì‚°ì—… = ë¡¯ë°ìì‚°ê°œë°œ : ? ]\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: ì‚¼ë¦½ì‹í’ˆ\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.13\n",
      "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\n",
      "------------------------------\n",
      "\n",
      "ê´€ê³„ [ ê·¹ë½ìœ¼ë¡œ : ì €ìŠ¹ìœ¼ë¡œ = ë°œí™€ë¡œ : ? ]\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: ì—°ê²½ìœ¼ë¡œ\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.08\n",
      "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\n",
      "------------------------------\n",
      "\n",
      "ê´€ê³„ [ ì˜†ì—ìˆê³  : ê¸¸ê±´ë„ˆì— = ìºë„ì‹œí‹°ê¹Œì§€ : ? ]\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: ìºë„ì‹œí‹°ì—ì„œ\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.21\n",
      "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\n",
      "------------------------------\n",
      "\n",
      "ê´€ê³„ [ ê³¼ì„¸í‘œì¤€ì€ : ë§¤ì…ì„¸ì•¡ì˜ = ì¢…í•©ì†Œë“ì„¸ë¥¼ : ? ]\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: ì¢…í•©ì†Œë“ì„¸ì˜\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.08\n",
      "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\n",
      "------------------------------\n",
      "\n",
      "ê´€ê³„ [ ìœ ì—”ì¸ê¶Œì´ì‚¬íšŒì— : ì¸ê¶Œì´ì‚¬íšŒì™€ = ì¸ê¶Œì´ì‚¬íšŒëŠ” : ? ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gensim.models import FastText\n",
    "import re\n",
    "\n",
    "print(\"ëŒ€ìš©ëŸ‰ FastText ëª¨ë¸('cc.ko.300.bin') ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(\"ì´ ê³¼ì •ì€ ìˆ˜ ë¶„ ì´ìƒ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "try:\n",
    "    model = FastText.load_fasttext_format('cc.ko.300.bin')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[ì˜¤ë¥˜] 'cc.ko.300.bin' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(\"ì •ì œëœ ëª…ì‚¬ ëª©ë¡ íŒŒì¼('filtered_noun_list.txt')ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n",
    "\n",
    "try:\n",
    "    with open('filtered_noun_list.txt', 'r', encoding='utf-8') as f:\n",
    "        filtered_vocab = []\n",
    "        for line in f:\n",
    "            filtered_vocab.append(line.strip())\n",
    "    print(f\"ëª©ë¡ ë¡œë”© ì™„ë£Œ! ì´ {len(filtered_vocab):,}ê°œì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[ì˜¤ë¥˜] 'filtered_noun_list.txt' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"ê²Œì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "while True:\n",
    "    a, b, c, predicted_d = None, None, None, None\n",
    "    retries = 0\n",
    "    while not all([a, b, c, predicted_d]) and retries < 100:\n",
    "        retries += 1\n",
    "        try:\n",
    "            a = random.choice(filtered_vocab)\n",
    "            similar_to_a = model.wv.most_similar(a, topn=50)\n",
    "\n",
    "            candidates_c = []\n",
    "            for word, sim in similar_to_a:\n",
    "                if sim > 0.5 and word in filtered_vocab and word != a:\n",
    "                    candidates_c.append(word)\n",
    "            \n",
    "            if not candidates_c:\n",
    "                continue\n",
    "            c = random.choice(candidates_c)\n",
    "\n",
    "            candidates_b = []\n",
    "            for word, sim in similar_to_a:\n",
    "                if sim > 0.6 and word in filtered_vocab and word != a and word != c:\n",
    "                    candidates_b.append(word)\n",
    "\n",
    "            if not candidates_b:\n",
    "                continue\n",
    "            b = random.choice(candidates_b)\n",
    "\n",
    "            top_predictions = model.wv.most_similar(positive=[c, b], negative=[a], topn=10)\n",
    "            \n",
    "            for word, _ in top_predictions:\n",
    "                if word in filtered_vocab:\n",
    "                    predicted_d = word\n",
    "                    break\n",
    "            \n",
    "            if predicted_d is None and top_predictions:\n",
    "                predicted_d = top_predictions[0][0]\n",
    "\n",
    "        except (KeyError, IndexError):\n",
    "            continue\n",
    "\n",
    "    if not predicted_d:\n",
    "        print(\"ì ì ˆí•œ ë¬¸ì œë¥¼ ë§Œë“œëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nê´€ê³„ [ {a} : {b} = {c} : ? ]\")\n",
    "    \n",
    "    user_d = input(\"ì •ë‹µì´ë¼ê³  ìƒê°í•˜ëŠ” ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "    if user_d not in model.wv:\n",
    "        print(f\"'{user_d}'ëŠ” ëª¨ë¸ì´ ì•Œì§€ ëª»í•˜ëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "        continue\n",
    "\n",
    "    similarity = model.wv.similarity(user_d, predicted_d)\n",
    "    print(f\"ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ ë‹¨ì–´: {predicted_d}\")\n",
    "    print(f\"ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ 1ìˆœìœ„ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: {similarity:.2f}\")\n",
    "\n",
    "    if similarity > 0.6:\n",
    "        print(\"ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰ 1ìˆœìœ„ ì˜ˆì¸¡ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë„¤ìš”!\")\n",
    "    elif similarity > 0.3:\n",
    "        print(\"ë¹„ìŠ·í•˜ë„¤ìš”! ì¡°ê¸ˆ ë” ìƒê°í•´ë³¼ê¹Œìš”? ğŸ¤”\")\n",
    "    else:\n",
    "        print(\"ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”. ğŸ˜¥\")\n",
    "    \n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
